# Install first if needed
# pip install imbalanced-learn

import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from sklearn.feature_selection import VarianceThreshold
from imblearn.over_sampling import SMOTE

# 1. Create an imbalanced dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5,
                           n_clusters_per_class=1, weights=[0.9], flip_y=0, random_state=42)

df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
df['target'] = y

# 2. Remove low-variance features
selector = VarianceThreshold(threshold=0.1)
X_reduced = selector.fit_transform(df.drop('target', axis=1))

# 3. Remove multicollinear features
X_df = pd.DataFrame(X_reduced)
corr_matrix = X_df.corr().abs()
upper_triangle = corr_matrix.where(
    pd.DataFrame([[i < j for j in range(corr_matrix.shape[1])] for i in range(corr_matrix.shape[0])])
)
to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]
X_df_clean = X_df.drop(X_df.columns[to_drop], axis=1)

# 4. Handle class imbalance with SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_df_clean, df['target'])

# 5. Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# 6. Decision Tree with pruning via GridSearchCV
param_grid = {
    'max_depth': [3, 5, 7, 10],
    'min_samples_split': [2, 5, 10]
}
dt = DecisionTreeClassifier(random_state=42)
grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='f1', n_jobs=-1)
grid_search.fit(X_train, y_train)

# 7. Evaluation
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
print(classification_report(y_test, y_pred))
